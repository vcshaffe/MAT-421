{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+E98452Mj2iOT2lqFmacJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vcshaffe/MAT-421/blob/main/ModuleD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Introduction\n",
        "\n",
        "Linear algebra is a field of mathematics used in various disciplines. Linear algebra uses several concepts like vector spaces, orthogonality, eigenvalues, matrix decomposition, linear regression and principal component analysis among many others.\n",
        "\n",
        "# 1.2 Elements of Linear Algebra\n",
        "\n",
        "Def 1.2.1 (Linear subspace) - A linear subspace of V is a subset U ⊆ V that is closed under vector addition and scalar multiplication. That is, for all u1 and u2 ⊆ U, u1+u2 ∊ U and au1,au2 ∊ U (a ∊ R).\n",
        "\n",
        "Def 1.2.2 (Span) - Let w1, ... , wm ∊ V. The span of {w1,...,wm}, or span(w1,...,wm) is the set of all linear combinations of the wj's.\n",
        "\n",
        "A list of vectors that span a linear subspace can also be referred to a spanning set of that linear subspace.\n",
        "\n",
        "Lemma 1.2.3 (Every Span is a Linear Subspace) - Let W = span(w1,...,wm). Then W is a linear subspace.\n",
        "\n",
        "Def 1.2.4 (Column Space) - Let A ∊ R^(nxm) be a nxm matrix with columns a1,...,am ∊ R^n. The column space of A, denoted col(A), is the span of the columns of A. col(A) = span(a1,...,am).\n",
        "\n",
        "Def 1.2.5 (Linear Independence) - A list of vectors u1,...,um is linearly independent if none of them can be written as a linear combination of the others. A list of vectors is called linearly dependent if not linearly independent.\n",
        "\n",
        "Def 1.2.7 (Basis of a space) - Let U be a linear subspace of V. A basis of U is a list of vectors u1,...,um in U that\n",
        "*   1) Span U\n",
        "*   2) Are linearly independent\n",
        "\n",
        "Thm 1.2.8 (Dimension Theorem) - Let U be a linear subspace of V. Any basis of U always has the same number of elements. All bases of U have the same length, or same number of elements. This is denoted dim(U).\n",
        "\n",
        " **1.2.2 Orthogonality**\n",
        "\n",
        " Def 1.2.11 - A list of vectors {u1,...,um} is orthonormal if the ui's are pairwise orthogonal and each has norm 1, <ui,uj> = 0 and ||ui|| = 1.\n",
        "\n",
        " Def 1.2.15 (Orthogonal Projection) - Let U ⊆ V be  a linear subspace with orthonormal basis q1,...,qm. The orthogonal projection of v ∊ V on U is defined as Puv = sum(j=1 to m), <v,qj>*qj.\n",
        "\n",
        " Thm 1.2.16 (Best Approximation Theorem) - Let U ⊆ V be a linear subspace with orthonormal basis q1,..,qm and let v ∊ V. For any u ∊ U, ||v - Puv|| ≤ ||v - u||.\n",
        "\n",
        " Lemma 1.2.17 (Pythagorean Theorem) - Let u,v ∊ V be orthogonal. Then ||u + v||^2 = ||u||^2 + ||v||^2.\n",
        "\n",
        " Lemma 1.2.18 (Cauchy-Schwarz) - For any u,v ∊ V, |<u,v>| ≤ ||u||*||v||.\n",
        "\n",
        " **1.2.3 Gram-Schmidt Process**\n",
        "\n",
        " Thm 1.2.20 (Gram-Schmidt) - Let a1,...,am in R^n be linearly independent. Then there exists an orthonormal basis q1,...,qm of span(a1,...,am).\n",
        "\n",
        " **1.2.4 Eigenvalues and Eigenvectors**\n",
        "\n",
        " Def 1.2.21 (Eigenvalues and Eigenvectors) - Let A ∊ R^(dxd) be a square matrix. Then λ ∊ R is an eigenvalue of A if there exists a nonzero vector x such that Ax = λx. The vector x is an eigenvector.\n",
        "\n",
        " Ex 1.2.24 (Diagonal and Similar Matrices) - Let A be similar to a matrix D = diag(λ1,...,λd) with distinct diagonal entries. That is, there exists nonsingular matrix P such that A = PDP^-1.\n",
        "\n",
        " Thm 1.2.25 - If A is symmetric, then any two eigenvectors from different eigenspaces are orthogonal.\n",
        "\n",
        " Thm 1.2.26 (Spectral Theorem for Symmetric Matrices) - An nxn symmetric matrix A has the following properties: \n",
        "\n",
        "*   A has n real eigenvalues, counting multiplicities.\n",
        "*   If λ is an eigenvalue of A with multiplicity k, then the eigenspace for λ is k-dimensional.\n",
        "*   The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.\n",
        "*   A is orthogonally diagonalizable.\n",
        "\n",
        "# 1.3 Linear Regression\n",
        "\n",
        "**1.3.1 QR Decomposition** - Useful procedure to solve a linear least squares problem using Gram-Schmidt."
      ],
      "metadata": {
        "id": "_XgdOo1-16CT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqJzBqH-0YqG",
        "outputId": "582df1af-a0a6-440e-9b0e-bd8a6a296c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: [[23 58]\n",
            " [42 18]]\n",
            "Q: [[-0.48031462 -0.87709627]\n",
            " [-0.87709627  0.48031462]]\n",
            "R: [[-47.88527958 -43.64598095]\n",
            " [  0.         -42.22592032]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "A = np.array([[23, 58], [42, 18]])\n",
        "Q, R = scipy.linalg.qr(A)\n",
        "\n",
        "print('A:', A)\n",
        "print('Q:', Q)\n",
        "print('R:', R)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thm 1.3.1 (Normal Equations) - Let A ∊ R^(nxm) be an nxm matrix with linearly independent columns and let b ∊ R^n be a vector. The solution to the least-squares problem min||Ax - b|| satisfies A^T  Ax = A^T  b, which are known as normal equations.\n",
        "\n",
        "Thm 1.3.2 (Least Squares via QR) - Let A ∊ R^(nxm) be an nxm matrix with linearly independent columns, let b ∊ R^n be a vector, and let A=QR be a QR decomposition of A, where Q is a R^(nxm) matrix with Q^T Q = I(mxm) and R is upper triangular. The solution to the least-squares problem min||Ax - b|| satisfies Rx* = Q^T b.\n",
        "\n",
        "**1.3.3 Linear Regression**\n",
        "\n",
        "Given input data points (xi,yi) from i=1 to n with each xi = (xi1,...,xid)^T, we seek an affline function to fit the data. The most common approach is finding coefficients βj that minimize the criterion sum(i=1 to n) (yi-yhati)^2, where yhati are the predicted values of the linear model with coefficients βj. \n",
        "\n",
        "The minimilization problem can then be formulated in matrix form, which then can be transformed to min||y - Aβ||^2, which is the least-squares problem which was previously solved."
      ],
      "metadata": {
        "id": "13BBx4ewCT2q"
      }
    }
  ]
}